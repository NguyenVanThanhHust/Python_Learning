{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import denseNetModel\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cifar10(batch_size, nb_epoch,depth, nb_dense_block, \n",
    "               nb_filter, growth_rate, dropout_rate, \n",
    "               learning_rate, weight_decay, plot_architure):\n",
    "    \"\"\"\n",
    "    Run CIFRA10 experiments\n",
    "    :param batch_size:int -- batch size\n",
    "    :param nb_epoch : int number epochs\n",
    "    :param depth: int network depth\n",
    "    :param nb_dense_block : int of dense block\n",
    "    :param nb_filter: int -- initial number of conv filter\n",
    "    :param growth_rate : int number of new filters added by conv layers\n",
    "    :param dropout_rate: float -- dropout rate\n",
    "    :param learning_rate: float -- learning rate\n",
    "    :param weight_decay: float -- weight decay\n",
    "    :param plot_architecture: bool -- whether to plot network architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    ###################\n",
    "    # Data processing #\n",
    "    ###################\n",
    "    \n",
    "    # the data, shuffled and split between train and test set\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    nb_classes = len(np.unique(y_train))\n",
    "    img_dim = X_train.shape[1:]\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        n_channels = X_train.shape[1]\n",
    "    else:\n",
    "        n_channels = X_train.shape[-1]\n",
    "        \n",
    "    #Convert class vector to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_train, nb_classes)\n",
    "    \n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    \n",
    "    # Normalisation\n",
    "    X = np.vstack((X_train, X_test))\n",
    "    # 2 cases depending on the image ordering\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        for i in range(n_channels):\n",
    "            mean = np.mean(X[:, i, :, :])\n",
    "            std = np.std(X[:, i, :, :])\n",
    "            X_train[:, i, :, :] = (X_train[:, i, :, :] - mean) / std\n",
    "            X_test[:, i, :, :] = (X_test[:, i, :, :] - mean) / std\n",
    "\n",
    "    elif K.image_data_format() == \"channels_last\":\n",
    "        for i in range(n_channels):\n",
    "            mean = np.mean(X[:, :, :, i])\n",
    "            std = np.std(X[:, :, :, i])\n",
    "            X_train[:, :, :, i] = (X_train[:, :, :, i] - mean) / std\n",
    "            X_test[:, :, :, i] = (X_test[:, :, :, i] - mean) / std\n",
    "            \n",
    "    ###################\n",
    "    # Construct model #\n",
    "    ###################\n",
    "    \n",
    "    model = densenet.DenseNet(nb_classes,\n",
    "                              img_dim,\n",
    "                              depth,\n",
    "                              nb_dense_block,\n",
    "                              growth_rate,\n",
    "                              nb_filter,\n",
    "                              dropout_rate=dropout_rate,\n",
    "                              weight_decay=weight_decay)\n",
    "    \n",
    "    # Model output\n",
    "    model.summary()\n",
    "\n",
    "    # Build optimizer\n",
    "    opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    if plot_architecture:\n",
    "        from keras.utils.visualize_util import plot\n",
    "        plot(model, to_file='./figures/densenet_archi.png', show_shapes=True)\n",
    "\n",
    "    ####################\n",
    "    # Network training #\n",
    "    ####################\n",
    "\n",
    "    print(\"Training\")\n",
    "\n",
    "    list_train_loss = []\n",
    "    list_test_loss = []\n",
    "    list_learning_rate = []\n",
    "\n",
    "    for e in range(nb_epoch):\n",
    "\n",
    "        if e == int(0.5 * nb_epoch):\n",
    "            K.set_value(model.optimizer.lr, np.float32(learning_rate / 10.))\n",
    "\n",
    "        if e == int(0.75 * nb_epoch):\n",
    "            K.set_value(model.optimizer.lr, np.float32(learning_rate / 100.))\n",
    "\n",
    "        split_size = batch_size\n",
    "        num_splits = X_train.shape[0] / split_size\n",
    "        arr_splits = np.array_split(np.arange(X_train.shape[0]), num_splits)\n",
    "\n",
    "        l_train_loss = []\n",
    "        start = time.time()\n",
    "\n",
    "        for batch_idx in arr_splits:\n",
    "\n",
    "            X_batch, Y_batch = X_train[batch_idx], Y_train[batch_idx]\n",
    "            train_logloss, train_acc = model.train_on_batch(X_batch, Y_batch)\n",
    "\n",
    "            l_train_loss.append([train_logloss, train_acc])\n",
    "\n",
    "        test_logloss, test_acc = model.evaluate(X_test,\n",
    "                                                Y_test,\n",
    "                                                verbose=0,\n",
    "                                                batch_size=64)\n",
    "        list_train_loss.append(np.mean(np.array(l_train_loss), 0).tolist())\n",
    "        list_test_loss.append([test_logloss, test_acc])\n",
    "        list_learning_rate.append(float(K.get_value(model.optimizer.lr)))\n",
    "        # to convert numpy array to json serializable\n",
    "        print('Epoch %s/%s, Time: %s' % (e + 1, nb_epoch, time.time() - start))\n",
    "\n",
    "        d_log = {}\n",
    "        d_log[\"batch_size\"] = batch_size\n",
    "        d_log[\"nb_epoch\"] = nb_epoch\n",
    "        d_log[\"optimizer\"] = opt.get_config()\n",
    "        d_log[\"train_loss\"] = list_train_loss\n",
    "        d_log[\"test_loss\"] = list_test_loss\n",
    "        d_log[\"learning_rate\"] = list_learning_rate\n",
    "\n",
    "        json_file = os.path.join('./log/experiment_log_cifar10.json')\n",
    "        with open(json_file, 'w') as fp:\n",
    "            json.dump(d_log, fp, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Run CIFAR10 experiment')\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--nb_epoch', default=30, type=int,\n",
    "                        help='Number of epochs')\n",
    "    parser.add_argument('--depth', type=int, default=7,\n",
    "                        help='Network depth')\n",
    "    parser.add_argument('--nb_dense_block', type=int, default=1,\n",
    "                        help='Number of dense blocks')\n",
    "    parser.add_argument('--nb_filter', type=int, default=16,\n",
    "                        help='Initial number of conv filters')\n",
    "    parser.add_argument('--growth_rate', type=int, default=12,\n",
    "                        help='Number of new filters added by conv layers')\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.2,\n",
    "                        help='Dropout rate')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1E-3,\n",
    "                        help='Learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1E-4,\n",
    "                        help='L2 regularization on weights')\n",
    "    parser.add_argument('--plot_architecture', type=bool, default=False,\n",
    "                        help='Save a plot of the network architecture')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Network configuration:\")\n",
    "    for name, value in parser.parse_args()._get_kwargs():\n",
    "        print(name, value)\n",
    "\n",
    "    list_dir = [\"./log\", \"./figures\"]\n",
    "    for d in list_dir:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "    run_cifar10(args.batch_size,\n",
    "                args.nb_epoch,\n",
    "                args.depth,\n",
    "                args.nb_dense_block,\n",
    "                args.nb_filter,\n",
    "                args.growth_rate,\n",
    "                args.dropout_rate,\n",
    "                args.learning_rate,\n",
    "                args.weight_decay,\n",
    "                args.plot_architecture)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
